{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Name: Chandeepa Janith\n",
        "\n",
        "Reg No: SKF2400091\n"
      ],
      "metadata": {
        "id": "JCDDgezfNbm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bigram Model**"
      ],
      "metadata": {
        "id": "I9TRDcgBw1VV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y_m4CdwFLtSm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "0agt0bBvMBT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters for the smaller model version\n",
        "B = 32  # batch size: how many independent sequences will we process in parallel?\n",
        "T = 8   # time: what is the maximum context length for predictions?\n",
        "C = 32  # Feature count: number of different features analyzed\n",
        "H = 4   # Number of attention heads\n",
        "L = 4   # Number of layers\n",
        "learning_rate = 1e-3  # Learning rate for the optimizer\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "dropout = 0.2\n",
        "torch.manual_seed(1337)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GwK7nrVL_2n",
        "outputId": "c9611a53-f3ce-4943-e01a-73e2284f23a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x783cde794990>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset"
      ],
      "metadata": {
        "id": "awrC5Bp9MHr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7o0QsKmMG-m",
        "outputId": "950e27ce-6e61-4487-a61c-a476984ed4d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-15 07:42:09--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-09-15 07:42:09 (19.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word-level tokenization"
      ],
      "metadata": {
        "id": "DNFRLGYSMOZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "\n",
        "# Load the text from 'input.txt'\n",
        "with open('input.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenize the text by splitting on spaces, newlines, or double newlines\n",
        "tokens = re.split(r'\\s+|\\n|\\n\\n', text)\n",
        "unique_words = sorted(set(tokens))  # Get unique tokens and sort them\n",
        "vocab_size = len(unique_words)      # Number of unique tokens\n",
        "word_to_index = {word: index for index, word in enumerate(unique_words)}  # Map words to indices\n",
        "index_to_word = {index: word for index, word in enumerate(unique_words)}  # Map indices to words\n",
        "\n",
        "# Functions to encode and decode text\n",
        "def encode_text(text):\n",
        "    return [word_to_index.get(word, -1) for word in re.split(r'\\s+|\\n|\\n\\n', text)]\n",
        "\n",
        "def decode_indices(indices):\n",
        "    return ' '.join([index_to_word.get(index, '<UNK>') for index in indices])\n",
        "\n",
        "# Display vocabulary details\n",
        "print(f'Vocabulary Size: {vocab_size}')\n",
        "print(f'Sample of Vocabulary: {unique_words[:50]}')  # Display the first 50 unique words\n",
        "\n",
        "# Convert text to tensor and split into training and validation datasets\n",
        "encoded_data = torch.tensor(encode_text(text), dtype=torch.long)\n",
        "split_index = int(0.9 * len(encoded_data))  # Use 90% for training and 10% for validation\n",
        "train_data = encoded_data[:split_index]\n",
        "val_data = encoded_data[split_index:]\n",
        "\n",
        "# Function to create batches of data\n",
        "def create_batch(split, batch_size, seq_length, device):\n",
        "    data_set = train_data if split == 'train' else val_data\n",
        "    indices = torch.randint(0, len(data_set) - seq_length, (batch_size,))\n",
        "    x = torch.stack([data_set[i:i + seq_length] for i in indices])\n",
        "    y = torch.stack([data_set[i + 1:i + seq_length + 1] for i in indices])\n",
        "    return x.to(device), y.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCyqg7RNMOnt",
        "outputId": "8ea96c6d-975e-49e0-a98a-a3cbe40d5161"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 25671\n",
            "Sample of Vocabulary: ['', '&C:', '&c.', \"'\", \"'?\", \"'A\", \"'Alas,\", \"'Alas,'\", \"'Alla\", \"'An\", \"'Ay\", \"'Ay,\", \"'Ay,'\", \"'Ay.'\", \"'Be\", \"'Beseech\", \"'Bless\", \"'Bove\", \"'Brutus!'\", \"'By\", \"'Charge\", \"'Charge!\", \"'Citizens!'\", \"'Clarence\", \"'Come\", \"'Come,\", \"'Commend\", \"'Con\", \"'Content'\", \"'Coriolanus!'\", \"'Courage!'\", \"'Courage,\", \"'Cucullus\", \"'Dear\", \"'Death.'\", \"'Deny\", \"'Do\", \"'Fair\", \"'Faith,\", \"'Farewell:'\", \"'Fine;'\", \"'Fore\", \"'Forgive\", \"'Frets,\", \"'G'\", \"'Gainst\", \"'Go\", \"'God\", \"'Good\", \"'Have\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single head of self-attention and MHSA self- attention"
      ],
      "metadata": {
        "id": "oNsPJ8sqMbpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "    \"\"\" Single head of self-attention mechanism \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.key_layer = nn.Linear(input_dim, output_dim, bias=False)\n",
        "        self.query_layer = nn.Linear(input_dim, output_dim, bias=False)\n",
        "        self.value_layer = nn.Linear(input_dim, output_dim, bias=False)\n",
        "        self.register_buffer('mask', torch.tril(torch.ones(T, T)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, input_dim = x.shape  # Extract dimensions\n",
        "\n",
        "        # Compute key, query, and value matrices\n",
        "        keys = self.key_layer(x)  # (batch_size, seq_len, output_dim)\n",
        "        queries = self.query_layer(x)  # (batch_size, seq_len, output_dim)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        attention_scores = torch.bmm(queries, keys.transpose(1, 2))  # (batch_size, seq_len, output_dim) @ (batch_size, output_dim, seq_len) -> (batch_size, seq_len, seq_len)\n",
        "        attention_scores /= input_dim ** 0.5  # Normalize by the square root of input_dim\n",
        "        attention_scores = attention_scores.masked_fill(self.mask[:seq_len, :seq_len] == 0, float('-inf'))  # Apply mask to upper triangle\n",
        "        attention_probs = F.softmax(attention_scores, dim=-1)  # Apply softmax to attention scores\n",
        "\n",
        "        values = self.value_layer(x)  # (batch_size, seq_len, output_dim)\n",
        "        output = torch.bmm(attention_probs, values)  # (batch_size, seq_len, seq_len) @ (batch_size, seq_len, output_dim) -> (batch_size, seq_len, output_dim)\n",
        "\n",
        "        return output\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\" Multi-head self-attention mechanism \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_heads, head_dim):\n",
        "        super().__init__()\n",
        "        self.attention_heads = nn.ModuleList([\n",
        "            AttentionHead(input_dim=input_dim, output_dim=head_dim) for _ in range(num_heads)\n",
        "        ])  # Create multiple attention heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Process input through all attention heads and concatenate their outputs\n",
        "        head_outputs = [head(x) for head in self.attention_heads]\n",
        "        concatenated_outputs = torch.cat(head_outputs, dim=-1)  # Concatenate along the last dimension\n",
        "        return concatenated_outputs\n"
      ],
      "metadata": {
        "id": "lje418IqMb0d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Block with multi-head attention and feedforward layers"
      ],
      "metadata": {
        "id": "GGo7aVxJMiku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\" Transformer block consisting of multi-head self-attention and feedforward layers \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_heads, dropout_rate):\n",
        "        super().__init__()\n",
        "        head_dim = input_dim // num_heads\n",
        "        self.multihead_attention = MultiHeadSelfAttention(input_dim, num_heads, head_dim)\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(input_dim, 4 * input_dim),\n",
        "            nn.GELU(),  # Using GeLU activation function\n",
        "            nn.Linear(4 * input_dim, input_dim),\n",
        "            nn.Dropout(dropout_rate),\n",
        "        )\n",
        "        self.layer_norm1 = nn.LayerNorm(input_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply multi-head attention with residual connection and layer normalization\n",
        "        attention_out = self.multihead_attention(self.layer_norm1(x))\n",
        "        x = x + attention_out\n",
        "\n",
        "        # Apply feedforward layer with residual connection and layer normalization\n",
        "        feedforward_out = self.feedforward(self.layer_norm2(x))\n",
        "        x = x + feedforward_out\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "5r6xtkXvMivd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigram language model with self-attention"
      ],
      "metadata": {
        "id": "sMKIkg62Mm3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    \"\"\" A Bigram Language Model with token and position embeddings, transformer blocks, and a final linear layer \"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, seq_length, embed_dim, num_heads, num_layers):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_length = seq_length\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Token and position embeddings\n",
        "        self.token_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_embeddings = nn.Embedding(seq_length, embed_dim)\n",
        "\n",
        "        # Stack of transformer blocks\n",
        "        self.transformer_blocks = nn.Sequential(*[Block(embed_dim, num_heads) for _ in range(num_layers)])\n",
        "\n",
        "        # Layer normalization and final linear layer\n",
        "        self.final_layer_norm = nn.LayerNorm(embed_dim)\n",
        "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # Get token and position embeddings\n",
        "        token_embeds = self.token_embeddings(idx)  # (batch_size, seq_length, embed_dim)\n",
        "        position_embeds = self.position_embeddings(torch.arange(self.seq_length, device=idx.device))  # (seq_length, embed_dim)\n",
        "\n",
        "        # Combine token and position embeddings\n",
        "        x = token_embeds + position_embeds  # (batch_size, seq_length, embed_dim)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        x = self.transformer_blocks(x)  # (batch_size, seq_length, embed_dim)\n",
        "\n",
        "        # Apply final layer normalization\n",
        "        x = self.final_layer_norm(x)  # (batch_size, seq_length, embed_dim)\n",
        "\n",
        "        # Compute logits for each token position\n",
        "        logits = self.output_layer(x)  # (batch_size, seq_length, vocab_size)\n",
        "\n",
        "        # Compute loss if targets are provided\n",
        "        if targets is not None:\n",
        "            logits = logits.view(-1, vocab_size)  # Flatten to (batch_size * seq_length, vocab_size)\n",
        "            targets = targets.view(-1)  # Flatten to (batch_size * seq_length)\n",
        "            loss = F.cross_entropy(logits, targets)  # Compute cross-entropy loss\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\" Generate new tokens given a starting sequence \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.seq_length:]  # Use the last `seq_length` tokens\n",
        "            logits, _ = self(idx_cond)  # Get logits for the current sequence\n",
        "            logits = logits[:, -1, :]  # Focus on the logits for the next token\n",
        "            probs = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # Sample next token\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # Append new token to the sequence\n",
        "\n",
        "        return idx\n",
        "\n",
        "# Instantiate and move model to the specified device\n",
        "model = BigramLanguageModel(batch_size=B, seq_length=T, embed_dim=C, num_heads=H, num_layers=L)\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "RUKyzROtMpaE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Estimate loss function"
      ],
      "metadata": {
        "id": "xxIE8Y30Mylc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def get_batch(split, batch_size, sequence_length, device):\n",
        "    \"\"\"Generate a batch of data for training or validation.\"\"\"\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    indices = torch.randint(0, len(data) - sequence_length, (batch_size,))\n",
        "    x = torch.stack([data[i:i + sequence_length] for i in indices])\n",
        "    y = torch.stack([data[i + 1:i + sequence_length + 1] for i in indices])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    \"\"\"Estimate the average loss on training and validation datasets.\"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, batch_size=B, sequence_length=T, device=device)  # Adjusted to include necessary arguments\n",
        "            logits, loss = model(X, Y)  # Forward pass and compute loss\n",
        "            losses[k] = loss.item()  # Store loss\n",
        "        out[split] = losses.mean()  # Compute average loss\n",
        "    model.train()  # Set model back to training mode\n",
        "    return out\n",
        "\n",
        "# Set up the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for iteration in range(max_iters):\n",
        "    # Periodically evaluate the loss on training and validation datasets\n",
        "    if iteration % eval_interval == 0:\n",
        "        losses = estimate_loss()  # Compute and print the average losses\n",
        "        print(f\"Step {iteration}: Train Loss = {losses['train']:.4f}, Val Loss = {losses['val']:.4f}\")\n",
        "\n",
        "    # Perform a training step\n",
        "    xb, yb = get_batch('train', batch_size=B, sequence_length=T, device=device)  # Adjusted to include necessary arguments\n",
        "    logits, loss = model(xb, yb)  # Compute the forward pass and loss\n",
        "    optimizer.zero_grad(set_to_none=True)  # Clear old gradients\n",
        "    loss.backward()  # Backpropagate the loss\n",
        "    optimizer.step()  # Update the model parameters\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVeDrJO3MzYl",
        "outputId": "ff5d466e-31b3-4e03-b6d8-9a8134b6de8b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Train Loss = 10.3110, Val Loss = 10.3111\n",
            "Step 500: Train Loss = 7.6186, Val Loss = 8.0379\n",
            "Step 1000: Train Loss = 7.3869, Val Loss = 7.9309\n",
            "Step 1500: Train Loss = 7.1510, Val Loss = 7.9031\n",
            "Step 2000: Train Loss = 6.9346, Val Loss = 7.8515\n",
            "Step 2500: Train Loss = 6.7392, Val Loss = 7.8452\n",
            "Step 3000: Train Loss = 6.6016, Val Loss = 7.8182\n",
            "Step 3500: Train Loss = 6.4349, Val Loss = 7.8920\n",
            "Step 4000: Train Loss = 6.3246, Val Loss = 7.9458\n",
            "Step 4500: Train Loss = 6.1783, Val Loss = 7.9822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ],
      "metadata": {
        "id": "KbFzoKpNNCys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"word_level_gpt.pth\")"
      ],
      "metadata": {
        "id": "qC5d1peXNDoN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test text generation"
      ],
      "metadata": {
        "id": "U1kLRyi2NGBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure context length is at least T\n",
        "context = torch.zeros((1, T), dtype=torch.long, device=device)  # Adjust context length to T\n",
        "print(decode(m.generate(context, max_new_tokens=50)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byhKr3OHNGck",
        "outputId": "9793afba-fd74-497c-ff64-43a610dbc096"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        graced You? breath: strange. Ho! rivals wasted, cheeks. kills Touching confusion's suitors. urged! lords: cast monument! impregnable, thou! Paulina! arms? nutmegs, necessaries guess flay'd? hairless limping swear remission with! rehearse, baffled greet jointure. stirs: above home-bred decreed, grow: before? children, And, amazed. cell philosophy. bottle. Than many's rage. they're own,--be\n"
          ]
        }
      ]
    }
  ]
}